{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Notebook Dir: /home/mangabat/projects/portofolio/backend/aiml_models/agent_teams/agent_tailored_cover_letter/notebook\n",
      "Project Root (agent_tailored_cover_letter): /home/mangabat/projects/portofolio/backend/aiml_models/agent_teams/agent_tailored_cover_letter\n",
      "Adding SRC_DIR to sys.path: /home/mangabat/projects/portofolio/backend/aiml_models/agent_teams/agent_tailored_cover_letter/src\n"
     ]
    }
   ],
   "source": [
    "# backend/aiml_models/agent_teams/agent_tailored_cover_letter/src/notebook/playground.ipynb\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Locate the \"src\" dynamically from notebook location\n",
    "NOTEBOOK_DIR = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(NOTEBOOK_DIR, \"..\"))\n",
    "\n",
    "SRC_DIR = os.path.join(PROJECT_ROOT, \"src\")\n",
    "\n",
    "# Debug prints to check\n",
    "print(f\"Current Notebook Dir: {NOTEBOOK_DIR}\")\n",
    "print(f\"Project Root (agent_tailored_cover_letter): {PROJECT_ROOT}\")\n",
    "print(f\"Adding SRC_DIR to sys.path: {SRC_DIR}\")\n",
    "\n",
    "sys.path.append(SRC_DIR)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "from typing import List, Literal, Dict\n",
    "\n",
    "class CorrectionsClient:\n",
    "    def __init__(self) -> None:\n",
    "        self.base_url = \"http://localhost:8010/corrections\"\n",
    "\n",
    "    def fetch_corrections(self, correction_type: Literal[\"word\", \"sentence\", \"skill\"]) -> List[Dict[str, str]]:\n",
    "        response = httpx.get(self.base_url, params={\"correction_type\": correction_type})\n",
    "        response.raise_for_status()\n",
    "        return response.json()  # Ensure it returns a list of dictionaries\n",
    "    \n",
    "skills = CorrectionsClient().fetch_corrections(\"skill\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_description = \"\"\"\n",
    "Data Scientist til Fraud Detection & AI Solutions\n",
    "Kan du dykke dybt i data? Har du styr på Machine Learning-modeller, SQL og Python? Og vil du arbejde med landets mest interessante datagrundlag? Så er du den Data Scientist, vi søger til afdelingen Fraud Detection & AI Solutions.\n",
    "Vil du være med til at fremtidssikre velfærdssamfundet med teknologier som ML, NLP og Computer Vision?\n",
    "I Digital Solutions får vi velfærden til at fungere. Vi spiller en afgørende rolle i digitaliseringen af det danske samfund, fordi vi står bag de systemer, der får to ud af tre velfærdskroner ud til danskerne. \n",
    "Her er vi lige nu på udkig efter en Data Scientist til afdelingen Fraud Detection & AI Solutions, hvor vi bl.a. sikrer en koordineret indsats i forhold til kontrol af fejludbetalinger og snyd med offentlige ydelser. \n",
    "Derudover spiller vi en vigtig rolle i arbejdet med effektivisering af forretningens processer ved brug af teknologier som Machine Learning, NLP, Computer vision og lignende teknologier. Siden sidste år har vi kortlagt et (kæmpe)stort NLP-potentiale på tværs af ATP, og nu arbejder vi på at udvikle og implementere løsningerne, så vi sikrer en effektiv udbetaling.\n",
    "I jobbet som Data Scientist er det mere konkret dig, der: \n",
    "Forstår forretningens behov på tæt hold.\n",
    "For at udvikle de bedste løsninger er det vigtigt at forstå forretningens behov. Derfor kommer du til at arbejde tæt sammen med forskellige teams for at afdække deres behov og omsætte dem til datadrevne løsninger.\n",
    "Udvikler avancerede modeller i Python\n",
    "Du er med til at udvikle statistiske modeller i Python, der er baseret på Machine Learning ved brug af både træningsdata og unsupervised metoder.\n",
    "Vedligeholder og monitorerer vores modeller\n",
    "For at sikre, at vores modeller fungerer optimalt, bliver du ansvarlig for løbende monitorering og vedligeholdelse. Her holder bl.a. øje med, hvorvidt modellerne opfører sig som forventet og tilpasser dem efter behov.\n",
    "Er med fra udvikling til produktion\n",
    "Vi arbejder med best practices fra softwareudvikling for at gøre overgangen fra udvikling til produktion så gnidningsfri som muligt. Du er derfor med til at sikre, at vores løsninger bringes sikkert fra udvikling til produktion i vores scrum-setup.\n",
    "Udvikler og implementerer NLP-løsninger\n",
    "Sidst, men ikke mindst, kommer du til at spille en central rolle i udviklingen af NLP-løsninger og sørger for, at de bliver produktionssat og taget i brug af vores interne kunder.\n",
    "Har du styr på SQL og Python?\n",
    "Der kan være flere veje til rollen som Data Scientist, men vi forestiller os, at du har en relevant kandidatgrad inden for matematik, statistik, fysik, computer science, ingeniørvidenskab eller lignende. Hvis du har et par års erfaring, er det en fordel.\n",
    "Derudover er du:\n",
    "erfaren når det kommer til dataanalyse og udvikling af Machine Learning-modeller\n",
    "en haj til Python, SQL og lignende (en fordel, ikke et krav)\n",
    "med til at skabe resultater i samarbejde med andre\n",
    "god til at finde enkle løsninger på komplekse udfordringer.\n",
    "Vil du være en del af et unikt it-fagligt fællesskab?\n",
    "I Fraud Detection & AI er vi en del af enheden Data i Digital Solutions. Vi brænder for data og de muligheder, data kan skabe. Som landets største udbetalingshus administrerer ATP to ud af tre velfærdskroner i Danmark. \n",
    "Vi arbejder med et unikt datagrundlag, som giver os særlige muligheder – både for at lave dybdegående analyser og for at udvikle datadrevne løsninger.\n",
    "Du bliver en del af en afdeling med mere end 40 dygtige kolleger, der arbejder som Data Scientists, Software Developers, Data Analysts og forretningsansvarlige. Du kommer især til at arbejde tæt sammen med afdelingens dygtige tekniske specialister, som arbejder med Data Science og softwareudvikling i full-stack-løsninger. \n",
    "Vi arbejder i et fagligt stærkt miljø, hvor vi deler viden og hjælper hinanden med at udvikle os – både teknisk og personligt.\n",
    "I ATP er barren sat højt, både når det gælder ambitioner og trivsel. Vi tror på et arbejdsliv i balance. Det kræver fleksibilitet med plads til den enkelte - og det har vi.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.completions.Completions object at 0x7fda536289a0> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fda5362b2b0> root_client=<openai.OpenAI object at 0x7fda7436f2e0> root_async_client=<openai.AsyncOpenAI object at 0x7fda53628a00> model_name='gpt-4o-2024-11-20' model_kwargs={} openai_api_key=SecretStr('**********') seed=66\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "from typing import List\n",
    "from langchain.schema import BaseMessage\n",
    "\n",
    "class LLMClient:\n",
    "    \"\"\"\n",
    "    Purpose:\n",
    "        This client wraps Langchain's ChatOllama to invoke local LLMs via Ollama.\n",
    "\n",
    "    Capabilities:\n",
    "        - Accepts system + human messages using Langchain schema.\n",
    "        - Uses Langchain's Ollama integration directly — no manual API calls.\n",
    "        - Returns the final response as plain text (ready for parsing).\n",
    "\n",
    "    Reasoning:\n",
    "        - This keeps you aligned with Langchain’s native event flow.\n",
    "        - You benefit from any future Langchain enhancements to `ChatOllama`.\n",
    "        - You avoid any API version mismatches between you and Ollama.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model: str ) -> None:\n",
    "        self.llm = ChatOllama(\n",
    "            model=model,\n",
    "            format=\"json\"\n",
    "            )\n",
    "\n",
    "model = ChatOllama(model=\"deepseek-r1:1.5b\")\n",
    "# class_model = LLMClient(model=\"deepseek-r1:1.5b\")\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm_model = ChatOpenAI(\n",
    "    api_key=None,\n",
    "    seed = 66,\n",
    "    model=\"gpt-4o-2024-11-20\"\n",
    ")\n",
    "\n",
    "print((llm_model))\n",
    "# print(type(class_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backend/aiml_models/agent_teams/agent_tailored_cover_letter/src/core/data_models/analysis_result_model.py\n",
    "\n",
    "from typing import Dict\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class JobAnalysisResult(BaseModel):\n",
    "    company_name: str = Field(description=\"Identified company name\")\n",
    "    job_title: str = Field(description=\"Identified job title\")\n",
    "    analysis_output: str = Field(description=\"Analysis of the vacancy\")\n",
    "    employees_skills_requirement: dict = Field(description=\"identified skills and technical experience required for the job vacancy\")\n",
    "    matching_skills: dict = Field(description=\"matching skills in the job vacancy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['my_skills'], input_types={}, partial_variables={'format_instructions': 'The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"company_name\": {\"description\": \"Identified company name\", \"title\": \"Company Name\", \"type\": \"string\"}, \"job_title\": {\"description\": \"Identified job title\", \"title\": \"Job Title\", \"type\": \"string\"}, \"analysis_output\": {\"description\": \"Analysis of the vacancy\", \"title\": \"Analysis Output\", \"type\": \"string\"}, \"employees_skills_requirement\": {\"description\": \"identified skills and technical experience required for the job vacancy\", \"title\": \"Employees Skills Requirement\", \"type\": \"object\"}, \"matching_skills\": {\"description\": \"matching skills in the job vacancy\", \"title\": \"Matching Skills\", \"type\": \"object\"}}, \"required\": [\"company_name\", \"job_title\", \"analysis_output\", \"employees_skills_requirement\", \"matching_skills\"]}\\n```'}, template=\"\\n        You are an AI assistant specializing in HR job analysis.\\n        Your task is to analyze a given job vacancy and match it with a candidate's skills.\\n        - Identified relevant skills from the job description.\\n        - Match the required skills with the candidate’s skills.\\n        - Assess the candidate's suitability for the role.\\n        - Identify:\\n            - The company name.\\n            - The job title.\\n            - Required skills and technical experience (stored as a dictionary).\\n            - Matching skills (stored as a dictionary).\\n        - Write a detailed analysis of the job vacancy and the candidate's skills on a one pager\\n        The candidate's skills are:\\n        {my_skills}\\n\\n        {format_instructions}\\n        \"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['job_position'], input_types={}, partial_variables={'format_instructions': 'The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"company_name\": {\"description\": \"Identified company name\", \"title\": \"Company Name\", \"type\": \"string\"}, \"job_title\": {\"description\": \"Identified job title\", \"title\": \"Job Title\", \"type\": \"string\"}, \"analysis_output\": {\"description\": \"Analysis of the vacancy\", \"title\": \"Analysis Output\", \"type\": \"string\"}, \"employees_skills_requirement\": {\"description\": \"identified skills and technical experience required for the job vacancy\", \"title\": \"Employees Skills Requirement\", \"type\": \"object\"}, \"matching_skills\": {\"description\": \"matching skills in the job vacancy\", \"title\": \"Matching Skills\", \"type\": \"object\"}}, \"required\": [\"company_name\", \"job_title\", \"analysis_output\", \"employees_skills_requirement\", \"matching_skills\"]}\\n```'}, template='\\n        Here is a job description that needs analysis:\\n        Job Vacancy: \\n        {job_position}\\n        \\n        {format_instructions}\\n        '), additional_kwargs={})]\n",
      "<class 'langchain_core.prompts.chat.ChatPromptTemplate'>\n",
      "input_variables=['job_position', 'my_skills'] input_types={} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['my_skills'], input_types={}, partial_variables={'format_instructions': 'The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"company_name\": {\"description\": \"Identified company name\", \"title\": \"Company Name\", \"type\": \"string\"}, \"job_title\": {\"description\": \"Identified job title\", \"title\": \"Job Title\", \"type\": \"string\"}, \"analysis_output\": {\"description\": \"Analysis of the vacancy\", \"title\": \"Analysis Output\", \"type\": \"string\"}, \"employees_skills_requirement\": {\"description\": \"identified skills and technical experience required for the job vacancy\", \"title\": \"Employees Skills Requirement\", \"type\": \"object\"}, \"matching_skills\": {\"description\": \"matching skills in the job vacancy\", \"title\": \"Matching Skills\", \"type\": \"object\"}}, \"required\": [\"company_name\", \"job_title\", \"analysis_output\", \"employees_skills_requirement\", \"matching_skills\"]}\\n```'}, template=\"\\n        You are an AI assistant specializing in HR job analysis.\\n        Your task is to analyze a given job vacancy and match it with a candidate's skills.\\n        - Identified relevant skills from the job description.\\n        - Match the required skills with the candidate’s skills.\\n        - Assess the candidate's suitability for the role.\\n        - Identify:\\n            - The company name.\\n            - The job title.\\n            - Required skills and technical experience (stored as a dictionary).\\n            - Matching skills (stored as a dictionary).\\n        - Write a detailed analysis of the job vacancy and the candidate's skills on a one pager\\n        The candidate's skills are:\\n        {my_skills}\\n\\n        {format_instructions}\\n        \"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['job_position'], input_types={}, partial_variables={'format_instructions': 'The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"company_name\": {\"description\": \"Identified company name\", \"title\": \"Company Name\", \"type\": \"string\"}, \"job_title\": {\"description\": \"Identified job title\", \"title\": \"Job Title\", \"type\": \"string\"}, \"analysis_output\": {\"description\": \"Analysis of the vacancy\", \"title\": \"Analysis Output\", \"type\": \"string\"}, \"employees_skills_requirement\": {\"description\": \"identified skills and technical experience required for the job vacancy\", \"title\": \"Employees Skills Requirement\", \"type\": \"object\"}, \"matching_skills\": {\"description\": \"matching skills in the job vacancy\", \"title\": \"Matching Skills\", \"type\": \"object\"}}, \"required\": [\"company_name\", \"job_title\", \"analysis_output\", \"employees_skills_requirement\", \"matching_skills\"]}\\n```'}, template='\\n        Here is a job description that needs analysis:\\n        Job Vacancy: \\n        {job_position}\\n        \\n        {format_instructions}\\n        '), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "# backend/aiml_models/agent_teams/agent_tailored_cover_letter/src/core/company_analysis/components/analysis_prompt_builder.py\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "\n",
    "class JobAnalysisResult(BaseModel):\n",
    "    company_name: str = Field(description=\"Identified company name\")\n",
    "    job_title: str = Field(description=\"Identified job title\")\n",
    "    analysis_output: str = Field(description=\"Analysis of the vacancy\")\n",
    "    employees_skills_requirement: dict = Field(description=\"identified skills and technical experience required for the job vacancy\")\n",
    "    matching_skills: dict = Field(description=\"matching skills in the job vacancy\")\n",
    "\n",
    "\n",
    "\n",
    "def build_prompt(jd, s, parser: PydanticOutputParser) -> ChatPromptTemplate:\n",
    "\n",
    "        # System message enforcing JSON output\n",
    "        system_analysis_template_str = \"\"\"\n",
    "        You are an AI assistant specializing in HR job analysis.\n",
    "        Your task is to analyze a given job vacancy and match it with a candidate's skills.\n",
    "        - Identified relevant skills from the job description.\n",
    "        - Match the required skills with the candidate’s skills.\n",
    "        - Assess the candidate's suitability for the role.\n",
    "        - Identify:\n",
    "            - The company name.\n",
    "            - The job title.\n",
    "            - Required skills and technical experience (stored as a dictionary).\n",
    "            - Matching skills (stored as a dictionary).\n",
    "        - Write a detailed analysis of the job vacancy and the candidate's skills on a one pager\n",
    "        The candidate's skills are:\n",
    "        {my_skills}\n",
    "\n",
    "        {format_instructions}\n",
    "        \"\"\"\n",
    "\n",
    "        SYSTEM_PROMPT = SystemMessagePromptTemplate(\n",
    "            prompt=PromptTemplate(\n",
    "                template=system_analysis_template_str,\n",
    "                input_variables=[\"my_skills\"],  # No user input needed\n",
    "                partial_variables={\"format_instructions\": parser.get_format_instructions()}  # Enforce JSON format\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Human input message\n",
    "        human_analysis_template_str = \"\"\"\n",
    "        Here is a job description that needs analysis:\n",
    "        Job Vacancy: \n",
    "        {job_position}\n",
    "        \n",
    "        {format_instructions}\n",
    "        \"\"\"\n",
    "\n",
    "        HUMAN_PROMPT = HumanMessagePromptTemplate(\n",
    "            prompt=PromptTemplate(\n",
    "                template=human_analysis_template_str,\n",
    "                input_variables=[\"job_position\"],\n",
    "                partial_variables={\"format_instructions\": parser.get_format_instructions()}  # Enforce JSON format\n",
    "            )\n",
    "        )\n",
    "        # Ensure that messages are formatted BEFORE returning the ChatPromptTemplate\n",
    "        messages = [SYSTEM_PROMPT, HUMAN_PROMPT]\n",
    "        print(messages)\n",
    "        chat_prompt = ChatPromptTemplate(\n",
    "            messages=messages\n",
    "        )\n",
    "\n",
    "        derp = parser.get_format_instructions()\n",
    "\n",
    "        chat_prompt.format_messages(\n",
    "            job_position=jd,\n",
    "            my_skills=s,\n",
    "            format_messages=derp\n",
    "        )\n",
    "\n",
    "        return chat_prompt\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=JobAnalysisResult)\n",
    "\n",
    "\n",
    "prompt = build_prompt(job_description, skills, parser)\n",
    "print(type(prompt))\n",
    "print(prompt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "company_name='ATP' job_title='Data Scientist - Fraud Detection & AI Solutions' analysis_output=\"The job vacancy at ATP for a 'Data Scientist - Fraud Detection & AI Solutions' primarily focuses on leveraging data science techniques, machine learning, and NLP to improve fraud detection and optimize business processes. Key responsibilities include developing machine learning models, implementing and monitoring NLP solutions, and ensuring smooth production of these solutions through best practices in software development. The organization emphasizes collaboration within a multidisciplinary team and values flexibility and work-life balance. The candidate demonstrates a strong skills match for this role. They have experience with Python, SQL, machine learning, statistical modeling, and NLP, which align strongly with the requirements of the position. Their skills in problem-solving, adaptability, and teamwork further enhance their suitability for this dynamic and collaborative role.\" employees_skills_requirement={'Python': 'Expertise with Python for developing machine learning and statistical models.', 'SQL': 'Proficiency in SQL for data analysis.', 'Machine Learning': 'Experience developing and implementing machine learning models using both supervised and unsupervised methods.', 'NLP (Natural Language Processing)': 'Strong understanding and hands-on experience with NLP techniques.', 'Statistical Modeling': 'Ability to create and work with statistical models.', 'Computer Vision': 'Familiarity with computer vision technologies.', 'Collaboration': 'Skills in working with cross-functional teams to design and implement solutions.', 'Scrum/Software Development Best Practices': 'Knowledge of software development workflows to transition models into production.'} matching_skills={'Python': True, 'SQL': True, 'Machine Learning': True, 'NLP (Natural Language Processing)': True, 'Statistical Modeling': True, 'Problem Solving': True, 'Team Player': True, 'Adaptability': True, 'Scrum': True, 'Software Development': True} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "chain = prompt | llm_model | parser\n",
    "\n",
    "structured_llm = chain.invoke(\n",
    "    {\n",
    "        \"job_position\": job_description,\n",
    "        \"my_skills\": skills,\n",
    "    }\n",
    ")\n",
    "print(structured_llm,\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Title: Data Scientist - Fraud Detection & AI Solutions\n",
      "Analysis Output: The job vacancy at ATP for a 'Data Scientist - Fraud Detection & AI Solutions' primarily focuses on leveraging data science techniques, machine learning, and NLP to improve fraud detection and optimize business processes. Key responsibilities include developing machine learning models, implementing and monitoring NLP solutions, and ensuring smooth production of these solutions through best practices in software development. The organization emphasizes collaboration within a multidisciplinary team and values flexibility and work-life balance. The candidate demonstrates a strong skills match for this role. They have experience with Python, SQL, machine learning, statistical modeling, and NLP, which align strongly with the requirements of the position. Their skills in problem-solving, adaptability, and teamwork further enhance their suitability for this dynamic and collaborative role.\n",
      "Employees Skills Requirement: {'Python': 'Expertise with Python for developing machine learning and statistical models.', 'SQL': 'Proficiency in SQL for data analysis.', 'Machine Learning': 'Experience developing and implementing machine learning models using both supervised and unsupervised methods.', 'NLP (Natural Language Processing)': 'Strong understanding and hands-on experience with NLP techniques.', 'Statistical Modeling': 'Ability to create and work with statistical models.', 'Computer Vision': 'Familiarity with computer vision technologies.', 'Collaboration': 'Skills in working with cross-functional teams to design and implement solutions.', 'Scrum/Software Development Best Practices': 'Knowledge of software development workflows to transition models into production.'}\n",
      "Matching Skills: {'Python': True, 'SQL': True, 'Machine Learning': True, 'NLP (Natural Language Processing)': True, 'Statistical Modeling': True, 'Problem Solving': True, 'Team Player': True, 'Adaptability': True, 'Scrum': True, 'Software Development': True}\n"
     ]
    }
   ],
   "source": [
    "print(\"Job Title:\", structured_llm.job_title)\n",
    "print(\"Analysis Output:\", structured_llm.analysis_output)\n",
    "print(\"Employees Skills Requirement:\", structured_llm.employees_skills_requirement)\n",
    "print(\"Matching Skills:\", structured_llm.matching_skills)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlanggraph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcheckpoint\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msqlite\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SqliteSaver\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Node imports\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompany_analysis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph_nodes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnode_generate_vacancy_analysis\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m generate_vacancy_analysis\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompany_analysis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph_nodes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnode_get_skills\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_skills\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcover_letter\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph_nodes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnode_generate_cover_letter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m generate_cover_letter\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "from typing import TypedDict, List, Annotated, Dict, Any, Optional\n",
    "from langgraph.graph.message import add_messages, AnyMessage\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "import uuid\n",
    "from pprint import pprint\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "\n",
    "# Node imports\n",
    "from src.core.company_analysis.graph_nodes.node_generate_vacancy_analysis import generate_vacancy_analysis\n",
    "from src.core.company_analysis.graph_nodes.node_get_skills import get_skills\n",
    "from src.core.cover_letter.graph_nodes.node_generate_cover_letter import generate_cover_letter\n",
    "from src.core.cover_letter.graph_nodes.node_semantic_similarity import retrieve_best_matching_template\n",
    "from src.core.editorial.graph_nodes.node_user_in_the_loop import user_in_the_loop\n",
    "from src.core.editorial.graph_nodes.node_validate_and_correct_editorial import validate_and_correct_editorial\n",
    "from src.core.nlp_rules_classifier.graph_nodes.node_grammatical_rules_classifier import grammatical_rules_classifier\n",
    "from src.core.graph_master.initialize_graph import CoverLetterGraphState\n",
    "from src.core.editorial.graph_nodes.node_decide_editorial_correction import decide_editorial_next_step\n",
    "\n",
    "def build_master_graph():\n",
    "    \"\"\"\n",
    "    Builds the master LangGraph flow for generating a validated, personalized cover letter.\n",
    "\n",
    "    Steps:\n",
    "    - Extract user skills\n",
    "    - Retrieve best matching template (semantic similarity search)\n",
    "    - Analyze job vacancy\n",
    "    - Generate initial cover letter\n",
    "    - Run editorial correction loop until validation passes or max iterations\n",
    "    - (Future) Human-in-the-loop\n",
    "    - (Future) LaTeX + PDF generation\n",
    "\n",
    "    Returns:\n",
    "        LangGraph: Compiled execution graph\n",
    "    \"\"\"\n",
    "    graph_builder = StateGraph(CoverLetterGraphState)\n",
    "\n",
    "    # Add core generation steps\n",
    "    graph_builder.add_node(\"get_skills\", get_skills)\n",
    "    graph_builder.add_node(\"semantic_similarity\", retrieve_best_matching_template)\n",
    "    graph_builder.add_node(\"analyse_vacancy\", generate_vacancy_analysis)\n",
    "    graph_builder.add_node(\"generate_cover_letter\", generate_cover_letter)\n",
    "\n",
    "    # Editorial validation loop\n",
    "    # graph_builder.add_node(\"check_editorial_output\", check_editorial_generation)\n",
    "    graph_builder.add_node(\"validate_and_correct_editorial\", validate_and_correct_editorial)\n",
    "\n",
    "    # Optional future steps (commented out for now)\n",
    "    graph_builder.add_node(\"user_in_the_loop\", user_in_the_loop)  # TODO\n",
    "    # graph_builder.add_node(\"create_pdf\", lambda state: state)  # TODO\n",
    "\n",
    "    # Define edges\n",
    "    graph_builder.set_entry_point(\"get_skills\")\n",
    "    graph_builder.add_edge(\"get_skills\", \"semantic_similarity\")\n",
    "    graph_builder.add_edge(\"semantic_similarity\", \"analyse_vacancy\")\n",
    "    graph_builder.add_edge(\"analyse_vacancy\", \"generate_cover_letter\")\n",
    "    graph_builder.add_edge(\"generate_cover_letter\", \"validate_and_correct_editorial\")\n",
    "\n",
    "    # Conditional logic: Editorial validation loop or exit\n",
    "    graph_builder.add_conditional_edges(\n",
    "        \"validate_and_correct_editorial\",\n",
    "        decide_editorial_next_step,\n",
    "        path_map={\n",
    "            \"validate_and_correct_editorial\": \"validate_and_correct_editorial\",\n",
    "            \"human_in_the_loop\": \"user_in_the_loop\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    graph_builder.set_finish_point(END)\n",
    "    # graph_builder.add_edge(\"validate_and_correct_editorial\", \"check_editorial_output\")\n",
    "\n",
    "    # Generate a unique thread ID\n",
    "    unique_id = str(uuid.uuid4())\n",
    "\n",
    "    # Configuration for the graph run\n",
    "    config = {\n",
    "        \"configurable\": {\n",
    "            \"thread_id\": unique_id,\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Set up the memory and compile the graph\n",
    "    memory = SqliteSaver.from_conn_string(\":memory:\")\n",
    "    t = graph_builder.compile(checkpointer=memory, config=config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "portofolio_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
