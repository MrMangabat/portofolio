{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Notebook Dir: /home/mangabat/projects/portofolio/backend/aiml_models/agent_teams/agent_tailored_cover_letter\n",
      "Project Root (agent_tailored_cover_letter): /home/mangabat/projects/portofolio/backend/aiml_models/agent_teams\n",
      "Adding SRC_DIR to sys.path: /home/mangabat/projects/portofolio/backend/aiml_models/agent_teams/src\n"
     ]
    }
   ],
   "source": [
    "# backend/aiml_models/agent_teams/agent_tailored_cover_letter/src/notebook/playground.ipynb\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Locate the \"src\" dynamically from notebook location\n",
    "NOTEBOOK_DIR = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(NOTEBOOK_DIR, \"..\"))\n",
    "\n",
    "SRC_DIR = os.path.join(PROJECT_ROOT, \"src\")\n",
    "\n",
    "# Debug prints to check\n",
    "print(f\"Current Notebook Dir: {NOTEBOOK_DIR}\")\n",
    "print(f\"Project Root (agent_tailored_cover_letter): {PROJECT_ROOT}\")\n",
    "print(f\"Adding SRC_DIR to sys.path: {SRC_DIR}\")\n",
    "\n",
    "sys.path.append(SRC_DIR)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Section 1\n",
    "# # File: backend/aiml_models/agent_teams/agent_tailored_cover_letter/src/infrastructure/corrections_client.py\n",
    "\n",
    "# import httpx\n",
    "# from typing import List, Literal\n",
    "# from src.config.config_top_level import ConfigTopLevel\n",
    "\n",
    "# class CorrectionsClient:\n",
    "#     def __init__(self) -> None:\n",
    "#         self.config = ConfigTopLevel.load_config()\n",
    "#         self.base_url = \"http://localhost:8010/corrections\"\n",
    "\n",
    "#     def fetch_corrections(self, correction_type: Literal[\"word\", \"sentence\", \"skill\"]) -> List[str]:\n",
    "#         response = httpx.get(self.base_url, params={\"correction_type\": correction_type})\n",
    "#         response.raise_for_status()\n",
    "#         return response.json()  # This gives you a proper list of dicts\n",
    "\n",
    "# corrections_client = CorrectionsClient()\n",
    "# skills_response = [item[\"text\"] for item in corrections_client.fetch_corrections(\"skill\")]\n",
    "# # skillsets = [skill[\"text\"] for skill in skills_response]\n",
    "\n",
    "\n",
    "# print(f\"Skills Response: {skills_response}\")\n",
    "# for i in skills_response:\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Section 2\n",
    "# # File: backend/aiml_models/agent_teams/agent_tailored_cover_letter/src/core/company_analysis/components/analysis_response_parser.py\n",
    "# import json\n",
    "# from langchain_core.output_parsers import PydanticOutputParser\n",
    "# from src.core.data_models.analysis_result_model import JobAnalysisResult\n",
    "\n",
    "# class JobAnalysisResultParser:\n",
    "#     \"\"\"\n",
    "#     Purpose:\n",
    "#         Parses the raw LLM response into a structured JobAnalysisResult object.\n",
    "\n",
    "#     Capabilities:\n",
    "#         - Converts raw LLM output (usually a JSON string) into a JobAnalysisResult.\n",
    "#         - Uses PydanticOutputParser to leverage type enforcement.\n",
    "\n",
    "#     Reasoning:\n",
    "#         - Keeping parsing logic separate from data models ensures:\n",
    "#         âœ… Parsing can evolve (different LLM formats) without changing data contracts.\n",
    "#         âœ… Data models stay clean (no LLM-specific logic).\n",
    "#     \"\"\"\n",
    "#     def __init__(self) -> None:\n",
    "#         self.parser = PydanticOutputParser(pydantic_object=JobAnalysisResult)\n",
    "\n",
    "\n",
    "#     def parse(self, llm_response: str) -> JobAnalysisResult:\n",
    "#         \"\"\"\n",
    "#         Converts raw string response from LLM into structured JobAnalysisResult.\n",
    "\n",
    "#         Args:\n",
    "#             llm_response: Raw JSON string from LLM.\n",
    "\n",
    "#         Returns:\n",
    "#             JobAnalysisResult: Parsed and validated structured result.\n",
    "#         \"\"\"\n",
    "#         return self.parser.parse(llm_response)\n",
    "\n",
    "# # Instantiate the parser\n",
    "# response_parser = JobAnalysisResultParser()\n",
    "\n",
    "# # Sample valid JSON string (simulate what an LLM would return)\n",
    "# mock_llm_response = \"\"\"\n",
    "# {\n",
    "#     \"company_name\": \"AwesomeTech\",\n",
    "#     \"job_title\": \"Data Scientist\",\n",
    "#     \"analysis_output\": \"The position requires advanced data analysis and machine learning skills.\",\n",
    "#     \"employees_skills_requirement\": {\n",
    "#         \"Python\": true,\n",
    "#         \"Machine Learning\": true,\n",
    "#         \"PowerBI\": false\n",
    "#     },\n",
    "#     \"matching_skills\": {\n",
    "#         \"Python\": true,\n",
    "#         \"Machine Learning\": true\n",
    "#     }\n",
    "# }\n",
    "# \"\"\"\n",
    "\n",
    "# # Parse and inspect result\n",
    "# parsed_result = response_parser.parse(mock_llm_response)\n",
    "\n",
    "# print(\"\\nâœ… Parsed Job Analysis Result:\")\n",
    "# print(json.dumps(parsed_result.model_dump(), indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Section 3\n",
    "# # File: analysis_prompt_builder.py\n",
    "\n",
    "# from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate, PromptTemplate\n",
    "# from langchain_core.output_parsers import PydanticOutputParser\n",
    "# from src.core.data_models.analysis_result_model import JobAnalysisResult\n",
    "# from typing import List\n",
    "\n",
    "# class AnalysisPromptBuilder:\n",
    "#     \"\"\"\n",
    "#     Purpose:\n",
    "#         Builds the structured prompt for the LLM to analyze a job vacancy.\n",
    "\n",
    "#     Capabilities:\n",
    "#         - Constructs a system message with instructions.\n",
    "#         - Constructs a human message with the actual job description and skills.\n",
    "#         - Ensures output format matches JobAnalysisResult schema.\n",
    "\n",
    "#     Reasoning:\n",
    "#         Centralizing prompt logic ensures consistency across the agent flow.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self) -> None:\n",
    "#         self.parser = PydanticOutputParser(pydantic_object=JobAnalysisResult)\n",
    "\n",
    "#     def build_prompt(self, skillsets: List[str], job_to_apply: str) -> ChatPromptTemplate:\n",
    "#         \"\"\"\n",
    "#         Builds a ChatPromptTemplate ready to be sent to the LLM.\n",
    "\n",
    "#         Args:\n",
    "#             skillsets: List of user's skills fetched from service_cover_letter.\n",
    "#             job_to_apply: Raw job vacancy text.\n",
    "\n",
    "#         Returns:\n",
    "#             ChatPromptTemplate: Fully assembled LLM prompt.\n",
    "#         \"\"\"\n",
    "\n",
    "#         system_analysis_template_str = \"\"\"\n",
    "#         You are a senior HR analyst working for a career advisory platform.\n",
    "#         Your job is to analyze the provided job vacancy and output only valid JSON matching this schema:\n",
    "\n",
    "#         {\n",
    "#             \"company_name\": \"string\",\n",
    "#             \"job_title\": \"string\",\n",
    "#             \"analysis_output\": \"string\",\n",
    "#             \"employees_skills_requirement\": { \"string\": boolean },\n",
    "#             \"matching_skills\": { \"string\": boolean }\n",
    "#         }\n",
    "\n",
    "#         Do not add explanations, greetings, or anything else â€” only the JSON object itself.\n",
    "#         \"\"\"\n",
    "\n",
    "#         system_prompt = SystemMessagePromptTemplate(\n",
    "#             prompt=PromptTemplate(\n",
    "#                 template=system_analysis_template_str,\n",
    "#                 input_variables=[],\n",
    "#             )\n",
    "#         )\n",
    "\n",
    "#         human_analysis_template_str = \"\"\"\n",
    "#         Job Vacancy Description:\n",
    "#         {job_position}\n",
    "\n",
    "#         Candidate's Skills:\n",
    "#         {my_skills}\n",
    "#         \"\"\"\n",
    "\n",
    "#         human_prompt = HumanMessagePromptTemplate(\n",
    "#             prompt=PromptTemplate(\n",
    "#                 template=human_analysis_template_str,\n",
    "#                 input_variables=[\"job_position\", \"my_skills\"]\n",
    "#             )\n",
    "#         )\n",
    "\n",
    "#         return ChatPromptTemplate(messages=[system_prompt, human_prompt])\n",
    "    \n",
    "# # Instantiate & test in notebook\n",
    "\n",
    "# # Job description (normally from frontend form)\n",
    "# job_description = \"\"\"\n",
    "# We are seeking a Data Scientist at InnovativeAI.\n",
    "# The role requires Python, SQL, and Machine Learning expertise.\n",
    "# \"\"\"\n",
    "\n",
    "# # Instantiate & build the prompt\n",
    "# prompt_builder = AnalysisPromptBuilder()\n",
    "# prompt = prompt_builder.build_prompt(skills_response, job_description)\n",
    "\n",
    "# # Render to see what actually gets sent to LLM\n",
    "\n",
    "# print(\"\\nâœ… Final Analysis Prompt (ready for LLM):\")\n",
    "# for message in formatted_prompt:\n",
    "#     print(f\"\\n[{message.type.upper()}]\\n{message.content}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Section 4\n",
    "# # backend/aiml_models/agent_teams/agent_tailored_cover_letter/src/core/company_analysis/components/analysis_rules_validator.py\n",
    "# from typing import List, Dict, Any\n",
    "# from src.core.data_models.analysis_result_model import JobAnalysisResult\n",
    "\n",
    "# class AnalysisRulesValidator:\n",
    "#     \"\"\"\n",
    "#     Purpose:\n",
    "#         Validates the analysis result against forbidden words and sentences across multiple iterations.\n",
    "#         Supports progressive error tracking for self-correction.\n",
    "\n",
    "#     Capabilities:\n",
    "#         - Scans the analysis output for forbidden content.\n",
    "#         - Tracks all found issues per iteration.\n",
    "#         - Generates reflection instructions for self-correction.\n",
    "#         - Returns structured result instead of raising exceptions.\n",
    "\n",
    "#     Reasoning:\n",
    "#         This separates detection from logic flow.\n",
    "#         âœ… Easy to pass feedback into reflection loops.\n",
    "#         âœ… Supports time-series error analysis (how errors evolve over iterations).\n",
    "#     \"\"\"\n",
    "\n",
    "#     def validate(self, analysis_result: JobAnalysisResult, forbidden_words: List[str], forbidden_sentences: List[str], iteration: int) -> Dict[str, Any]:\n",
    "#         \"\"\"\n",
    "#         Validates the analysis output, tracks issues, and prepares correction feedback.\n",
    "\n",
    "#         Args:\n",
    "#             analysis_result: Structured result to validate.\n",
    "#             forbidden_words: List of forbidden words.\n",
    "#             forbidden_sentences: List of forbidden sentences.\n",
    "#             iteration: Current iteration number (for logging).\n",
    "\n",
    "#         Returns:\n",
    "#             Dict containing status, found issues, and reflection feedback.\n",
    "#         \"\"\"\n",
    "#         issues = self._check_forbidden_content(analysis_result.analysis_output, forbidden_words, forbidden_sentences)\n",
    "\n",
    "#         if not issues[\"words\"] and not issues[\"sentences\"]:\n",
    "#             return {\n",
    "#                 \"status\": \"passed\",\n",
    "#                 \"iteration\": iteration,\n",
    "#                 \"found_issues\": issues,\n",
    "#                 \"reflection\": None\n",
    "#             }\n",
    "\n",
    "#         reflection = self._generate_reflection(issues)\n",
    "\n",
    "#         return {\n",
    "#             \"status\": \"failed\",\n",
    "#             \"iteration\": iteration,\n",
    "#             \"found_issues\": issues,\n",
    "#             \"reflection\": reflection\n",
    "#         }\n",
    "\n",
    "#     def _check_forbidden_content(self, text: str, forbidden_words: List[str], forbidden_sentences: List[str]) -> Dict[str, List[str]]:\n",
    "#         \"\"\"\n",
    "#         Internal content scan, checks against words & sentences.\n",
    "#         \"\"\"\n",
    "#         text_lower = text.lower()\n",
    "\n",
    "#         found_words = [word for word in forbidden_words if word.lower() in text_lower]\n",
    "#         found_sentences = [sentence for sentence in forbidden_sentences if sentence.lower() in text_lower]\n",
    "\n",
    "#         return {\n",
    "#             \"words\": found_words,\n",
    "#             \"sentences\": found_sentences\n",
    "#         }\n",
    "\n",
    "#     def _generate_reflection(self, issues: Dict[str, List[str]]) -> str:\n",
    "#         \"\"\"\n",
    "#         Generates self-correction reflection instruction.\n",
    "#         \"\"\"\n",
    "#         reflection = []\n",
    "#         if issues[\"words\"]:\n",
    "#             reflection.append(f\"Remove or replace these words: {', '.join(issues['words'])}.\")\n",
    "#         if issues[\"sentences\"]:\n",
    "#             reflection.append(f\"Rephrase or remove these sentences: {', '.join(issues['sentences'])}.\")\n",
    "        \n",
    "#         reflection.append(\"Keep the meaning intact but ensure no forbidden language is present.\")\n",
    "#         return \" \".join(reflection)\n",
    "\n",
    "\n",
    "# from src.core.company_analysis.components.analysis_rules_validator import AnalysisRulesValidator\n",
    "# from src.core.data_models.analysis_result_model import JobAnalysisResult\n",
    "\n",
    "# validator = AnalysisRulesValidator()\n",
    "\n",
    "# mock_result = JobAnalysisResult(\n",
    "#     company_name=\"AwesomeTech\",\n",
    "#     job_title=\"Data Scientist\",\n",
    "#     analysis_output=\"This role is a perfect fit for someone eager to excel in machine learning.\",\n",
    "#     employees_skills_requirement={\"Python\": True, \"SQL\": True},\n",
    "#     matching_skills={\"Python\": True, \"SQL\": True}\n",
    "# )\n",
    "\n",
    "# forbidden_words = [\"perfect\", \"eager\", \"excel\"]\n",
    "# forbidden_sentences = [\"perfect fit for this role\"]\n",
    "\n",
    "# # Simulate first run\n",
    "# feedback = validator.validate(mock_result, forbidden_words, forbidden_sentences, iteration=1)\n",
    "# print(\"\\nâœ… Validator Feedback:\")\n",
    "# print(feedback)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # section 5\n",
    "# # backend/aiml_models/agent_teams/agent_tailored_cover_letter/src/core/company_analysis/agent_service_class_company_analysis.py\n",
    "\n",
    "\n",
    "# from typing import Dict, List\n",
    "# from src.infrastructure.correction_client import CorrectionsClient\n",
    "# from src.core.company_analysis.components.analysis_prompt_builder import AnalysisPromptBuilder\n",
    "# from src.core.company_analysis.components.analysis_respose_parser import JobAnalysisResultParser\n",
    "# from src.core.company_analysis.components.analysis_rules_validator import AnalysisRulesValidator\n",
    "# from src.core.data_models.analysis_result_model import JobAnalysisResult\n",
    "# from src.infrastructure.llm_client import LLMClient\n",
    "\n",
    "# class AgentServiceClassCompanyAnalysis:\n",
    "#     \"\"\"\n",
    "#     Purpose:\n",
    "#         Orchestrates the full flow of analyzing a job vacancy using the company analysis agent.\n",
    "\n",
    "#     Capabilities:\n",
    "#         - Fetch forbidden words, sentences, and skills from service_cover_letter.\n",
    "#         - Builds and sends a structured prompt to the LLM via LLMClient.\n",
    "#         - Parses the LLM response into structured data.\n",
    "#         - Validates the result against forbidden content rules.\n",
    "#         - Supports iterative self-correction if validation fails.\n",
    "\n",
    "#     Reasoning:\n",
    "#         This centralizes all flow logic into a single point, maintaining separation between:\n",
    "#         - Infrastructure (API calls, LLM calls)\n",
    "#         - Core logic (prompt, parsing, validation)\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         corrections_client: CorrectionsClient,\n",
    "#         prompt_builder: AnalysisPromptBuilder,\n",
    "#         response_parser: JobAnalysisResultParser,\n",
    "#         rules_validator: AnalysisRulesValidator,\n",
    "#         llm_client: LLMClient  # This MUST be present\n",
    "#         ) -> None:\n",
    "#         self.corrections_client = corrections_client\n",
    "#         self.prompt_builder = prompt_builder\n",
    "#         self.response_parser = response_parser\n",
    "#         self.rules_validator = rules_validator\n",
    "#         self.llm_client = llm_client\n",
    "#         self.correction_history: List[Dict] = []  # Tracks all corrections across iterations\n",
    "\n",
    "#     def analyze_job_vacancy(self, job_description: str) -> JobAnalysisResult:\n",
    "#         # Fetch forbidden words, sentences, and skills\n",
    "#         forbidden_words = [item[\"text\"] for item in self.corrections_client.fetch_corrections(\"word\")]\n",
    "#         forbidden_sentences = [item[\"text\"] for item in self.corrections_client.fetch_corrections(\"sentence\")]\n",
    "#         skillsets = [item[\"text\"] for item in self.corrections_client.fetch_corrections(\"skill\")]\n",
    "\n",
    "#         # Main generation loop with up to 12 self-correction attempts\n",
    "#         for iteration in range(1, 13):\n",
    "#             prompt = self.prompt_builder.build_prompt(skills_response, job_description)\n",
    "#             formatted_prompt = prompt.format_messages(job_position=job_description, my_skills=skills_response)\n",
    "\n",
    "#             # ðŸ”¥ Invoke the real LLM via Ollama\n",
    "#             raw_response = self.llm_client.invoke(formatted_prompt)\n",
    "\n",
    "#             # Parse response\n",
    "#             parsed_response = self.response_parser.parse(raw_response)\n",
    "\n",
    "#             # Validate response\n",
    "#             feedback = self.rules_validator.validate(parsed_response, forbidden_words, forbidden_sentences, iteration)\n",
    "\n",
    "#             if feedback[\"status\"] == \"passed\":\n",
    "#                 print(f\"âœ… Analysis passed after {iteration} iterations.\")\n",
    "#                 return parsed_response\n",
    "\n",
    "#             # Store feedback for future self-correction\n",
    "#             self.correction_history.append(feedback)\n",
    "\n",
    "#             # Prepare for next round with feedback (could modify prompt, but for now we log and retry as-is)\n",
    "#             self._log_feedback(feedback)\n",
    "\n",
    "#         raise ValueError(\"ðŸš¨ Analysis failed after 12 correction attempts.\")\n",
    "\n",
    "#     def _log_feedback(self, feedback: Dict) -> None:\n",
    "#         print(f\"\\nâŒ Iteration {feedback['iteration']} failed â€” Reflection for self-correction:\\n{feedback['reflection']}\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "input_variables=['\\n            \"company_name\"', 'job_position', 'my_skills'] input_types={} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['\\n            \"company_name\"'], input_types={}, partial_variables={}, template='\\n        You are a senior HR analyst working for a career advisory platform.\\n        Analyze the following job vacancy and output only valid JSON that strictly conforms to this schema:\\n\\n        \\n        {\\n            \"company_name\": \"string\",\\n            \"job_title\": \"string\",\\n            \"analysis_output\": \"string\",\\n            \"employees_skills_requirement\": { \"string\": boolean },\\n            \"matching_skills\": { \"string\": boolean }\\n        }\\n        \\n\\n        Important:\\n        - Do not add explanations, greetings, or comments.\\n        - Only return the JSON object itself â€” nothing else.\\n        '), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['job_position', 'my_skills'], input_types={}, partial_variables={}, template=\"\\n        Job Vacancy Description:\\n        {job_position}\\n\\n        Candidate's Skills:\\n        {my_skills}\\n        \"), additional_kwargs={})]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'type'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Trigger the full analysis process\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# try:\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m final_result \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manalyze_job_vacancy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob_description\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mâœ… Final Analysis Result (JobAnalysisResult):\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(final_result\u001b[38;5;241m.\u001b[39mmodel_dump_json(indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m))\n",
      "File \u001b[0;32m~/projects/portofolio/backend/aiml_models/agent_teams/agent_tailored_cover_letter/src/core/company_analysis/agent_service_class_company_analysis.py:63\u001b[0m, in \u001b[0;36mAgentServiceClassCompanyAnalysis.analyze_job_vacancy\u001b[0;34m(self, job_description)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(prompt)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# formatted_skills = \", \".join(skillsets)  # Convert list to comma-separated string\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# print(f\"Formatted skills: {formatted_skills}\")\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# formatted_prompt = prompt.format_messages(job_position=job_description, my_skills=formatted_skills)  # This line right here!\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# prompt.format_prompt(job_position=job_description, my_skills=formatted_skills)\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# print(f\"Formatted prompt for iteration {iteration}:\\n{formatted_prompt}\")\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# ðŸ”¥ Invoke the real LLM via Ollama\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m raw_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Parse response\u001b[39;00m\n\u001b[1;32m     66\u001b[0m parsed_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse_parser\u001b[38;5;241m.\u001b[39mparse(raw_response)\n",
      "File \u001b[0;32m~/projects/portofolio/backend/aiml_models/agent_teams/agent_tailored_cover_letter/src/infrastructure/llm_client.py:39\u001b[0m, in \u001b[0;36mLLMClient.invoke\u001b[0;34m(self, messages)\u001b[0m\n\u001b[1;32m     37\u001b[0m         converted_messages\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: message\u001b[38;5;241m.\u001b[39mtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: message\u001b[38;5;241m.\u001b[39mcontent})\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 39\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported message type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     41\u001b[0m payload \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: converted_messages,\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat\u001b[39m\u001b[38;5;124m\"\u001b[39m: JobAnalysisResult\u001b[38;5;241m.\u001b[39mmodel_json_schema(),  \u001b[38;5;66;03m# Attach schema directly\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     46\u001b[0m }\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;66;03m# \"role\": message.type,\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;66;03m# \"content\": message.content,\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'type'"
     ]
    }
   ],
   "source": [
    "# Section 6\n",
    "# File: backend/aiml_models/agent_teams/agent_tailored_cover_letter/src/agents_cover_letter_main.py\n",
    "\n",
    "# Full End-to-End Test - Notebook Section\n",
    "\n",
    "# from src.infrastructure.correction_client import CorrectionsClient\n",
    "# from src.infrastructure.llm_client import LLMClient\n",
    "# from src.core.company_analysis.components.analysis_prompt_builder import AnalysisPromptBuilder\n",
    "# from src.core.company_analysis.components.analysis_respose_parser import JobAnalysisResultParser\n",
    "# from src.core.company_analysis.components.analysis_rules_validator import AnalysisRulesValidator\n",
    "# from src.core.company_analysis.agent_service_class_company_analysis import AgentServiceClassCompanyAnalysis\n",
    "\n",
    "# Initialize dependencies\n",
    "corrections_client = CorrectionsClient()\n",
    "# llm_client = LLMClient(model=\"deepseek\")  # Adjust if using different model\n",
    "\n",
    "# Instantiate the full agent\n",
    "agent = AgentServiceClassCompanyAnalysis(\n",
    "    corrections_client=corrections_client,\n",
    "    prompt_builder=AnalysisPromptBuilder(),\n",
    "    response_parser=JobAnalysisResultParser(),\n",
    "    rules_validator=AnalysisRulesValidator(),\n",
    "    llm_client=LLMClient(model=\"deepseek-r1\")\n",
    ")\n",
    "\n",
    "# Example job description (this would normally come from frontend input)\n",
    "job_description = \"\"\"\n",
    "We are looking for a Data Scientist to join our team.\n",
    "The ideal candidate should have experience in Python, SQL, and Machine Learning.\n",
    "\"\"\"\n",
    "\n",
    "print()\n",
    "# Trigger the full analysis process\n",
    "# try:\n",
    "final_result = agent.analyze_job_vacancy(job_description)\n",
    "print(\"\\nâœ… Final Analysis Result (JobAnalysisResult):\")\n",
    "print(final_result.model_dump_json(indent=2))\n",
    "# except Exception as e:\n",
    "#     print(f\"\\nðŸš¨ Analysis Process Failed: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "portofolio_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
